{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0484a7ef",
   "metadata": {},
   "source": [
    "## Nederland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877898c0-4c6f-4311-87bc-2ac8069b54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b85aaa",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6a45ddcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to loop through\n",
    "urls =  [\"https://www.aldi.nl/zoeken.html?query=noten\",\n",
    "    \"https://www.aldi.nl/producten/chips--noten/noten--zaden-en-pitten.html\",\n",
    "    \"https://www.aldi.nl/producten/chips--noten/zoutjes.html\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=pitten\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=cashew\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=amandelen\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=trader+joe+pinda\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=dry+roasted\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=rozijn\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=macadamia\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=time4choco\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=rijstzoutjes\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=chocoladepinda\"]\n",
    "\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"Berrie.xlsx\"\n",
    "\n",
    "# Check if the Excel file already exists\n",
    "if os.path.exists(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)  # Load existing file\n",
    "    ws = wb.active\n",
    "else:\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Products\"\n",
    "    ws.append([\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\", \"Timestamp\"])  # Headers\n",
    "\n",
    "# Loop through the URLs and scrape data\n",
    "for url in urls:\n",
    "    driver.get(url)  # Navigate to the page first\n",
    "\n",
    "    # Wait for the products to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"product-tile__content\"))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Warning: No products found for {url}\")\n",
    "\n",
    "    # Get the page source after JavaScript renders it\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # List to hold product data\n",
    "    products = []\n",
    "\n",
    "    # Loop through all product tiles\n",
    "    for product in soup.find_all('div', class_='product-tile__content'):\n",
    "        # Extract product title\n",
    "        title_element = product.find('h2', class_='product-tile__content__upper__product-name')\n",
    "        title = title_element.get_text(strip=True) if title_element else 'Title not found'\n",
    "\n",
    "        # Extract current price\n",
    "        current_price_element = product.find('div', class_='product-tile__content__lower__wrapper__price-section__amount')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "\n",
    "        # Extract promo price (only the number, exclude percentage discount)\n",
    "        promo_price_element = product.find('p', class_='text product-tile__content__lower__wrapper__price-section__discount__striked')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'No promo price'\n",
    "\n",
    "        # Extract weight\n",
    "        weight_element = product.find('p', class_='product-tile__content__lower__wrapper__legal__text')\n",
    "        weight = weight_element.get_text(strip=True) if weight_element else 'Weight not found'\n",
    "\n",
    "        # Append product data\n",
    "        products.append((title, current_price, promo_price, weight, \"NL\", \"Aldi\"))\n",
    "\n",
    "    # Add timestamp\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Write product data to Excel\n",
    "    for product in products:\n",
    "        ws.append((*product, timestamp))\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_name)\n",
    "\n",
    "print(f\"Data successfully saved to {file_name}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d294d-15fe-4534-b44e-97622157a828",
   "metadata": {},
   "source": [
    "## Dirk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "19fa94ec-c165-4549-a563-5b84b9b9354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pop-ups found.\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Chrome driver with options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "url = \"https://www.dirk.nl/boodschappen/snacks-snoep/chocolade\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Initial load wait\n",
    "\n",
    "# Function to scroll down\n",
    "def scroll_to_load_more(driver, wait_time=2, scroll_increment=1200, scroll_limit=2):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(scroll_limit):\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "        time.sleep(wait_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "scroll_to_load_more(driver)\n",
    "\n",
    "# Function to safely click an element\n",
    "def safe_click(xpath):\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", element)\n",
    "        time.sleep(1)\n",
    "        element.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking {xpath}: {e}\")\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].click();\", element)  # JavaScript fallback\n",
    "        except:\n",
    "            print(f\"JavaScript click failed for {xpath}\")\n",
    "\n",
    "# Close pop-ups or overlays if present\n",
    "try:\n",
    "    close_button = WebDriverWait(driver, 5).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Accept')]\"))\n",
    "    )\n",
    "    close_button.click()\n",
    "    time.sleep(2)\n",
    "except:\n",
    "    print(\"No pop-ups found.\")\n",
    "\n",
    "# Click filters\n",
    "safe_click(\"//label[contains(text(), 'Overige chocolade & bonbons')]\")\n",
    "time.sleep(3)\n",
    "safe_click(\"//label[contains(text(), '1 de Beste')]\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Wait for products to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//article[@data-product-id]\"))\n",
    ")\n",
    "\n",
    "# Parse page content\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract product information\n",
    "products = []\n",
    "\n",
    "for article in soup.find_all('article', attrs={'data-product-id': True}):\n",
    "    title = article.find('p', class_='title').get_text(strip=True) if article.find('p', class_='title') else 'Title not found'\n",
    "    \n",
    "    price_integer = article.find('span', class_='price-large')\n",
    "    price_decimals = article.find('span', class_='price-small')\n",
    "    price = f\"{price_integer.get_text(strip=True)},{price_decimals.get_text(strip=True)}\" if price_integer and price_decimals else 'Price not found'\n",
    "\n",
    "    promo_price_span = article.find('div', class_='label price-label')\n",
    "    promo_price = promo_price_span.find('span', class_='regular-price').find('span').get_text(strip=True) if promo_price_span else 'Promo price not found'\n",
    "\n",
    "    weight_span = article.find('span', class_='subtitle')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Dirk\"))\n",
    "\n",
    "# Save to Excel\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "file_name = \"Berrie.xlsx\"\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    ws = wb.active\n",
    "else:\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Products\"\n",
    "    ws.append([\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Branded\", \"Retailer\", \"Timestamp\"])\n",
    "\n",
    "for product in products:\n",
    "    ws.append((*product, timestamp))\n",
    "\n",
    "wb.save(file_name)\n",
    "print(f\"Data has been successfully saved to {file_name}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e50c5-ad6c-4e15-8735-9f041cd257cf",
   "metadata": {},
   "source": [
    "## Vomar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69ee3a6f-42d0-4009-9261-2b11951ad672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accept cookies button found.\n",
      "No accept cookies button found.\n",
      "No accept cookies button found.\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.vomar.nl/zoeken?search=g%27woon%20choco\",\n",
    "    \"https://www.vomar.nl/zoeken?search=noten\",\n",
    "    \"https://www.vomar.nl/zoeken?search=pitten\",\n",
    "    \"https://www.vomar.nl/zoeken?search=rijstzoutjes\"\n",
    "]\n",
    "\n",
    "products = []\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for page to load\n",
    "    \n",
    "    # Click the \"Weigeren\" button to reject cookies on the Vomar site\n",
    "    try:\n",
    "        deny_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"CybotCookiebotDialogBodyButtonDecline\")))\n",
    "        deny_button.click()\n",
    "    except:\n",
    "        print(\"No accept cookies button found.\")\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Loop through all product articles\n",
    "    for article in soup.find_all('div', class_='col-xs-12 col-md-3 product'):\n",
    "        title = article.find('p', class_='description').get_text(strip=True) if article.find('p', class_='description') else 'Title not found'\n",
    "\n",
    "        price_integer = article.find('span', class_='large')\n",
    "        price_decimals = article.find('span', class_='small')\n",
    "        \n",
    "        if price_integer and price_decimals:\n",
    "            price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "        else:\n",
    "            price = 'Price not found'\n",
    "\n",
    "        promo_price = 'Promo price not found'\n",
    "        weight = 'Weight not found'\n",
    "        \n",
    "        products.append((title, price, promo_price, weight, \"NL\", \"Vomar\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"Berrie.xlsx\"\n",
    "\n",
    "# Check if the Excel file already exists\n",
    "if os.path.exists(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    ws = wb.active\n",
    "else:\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Products\"\n",
    "\n",
    "# Write product data to Excel\n",
    "for product in products:\n",
    "    ws.append((*product, timestamp))\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "wb.save(file_name)\n",
    "\n",
    "print(f\"Data has been successfully saved to {file_name}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683471e",
   "metadata": {},
   "source": [
    "## Duitsland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25a7d",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2405a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx.\n"
     ]
    }
   ],
   "source": [
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=asiatisce%20snack&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=kerne&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/sortiment/snacks-suessigkeiten/nuesse-trockenfruechte.html\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=trader%20joe%20n%C3%BCsse&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=trader%20joe%20mix&searchCategory=Submitted%20Search\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=schoko%20rosinen&searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&query=choceur%20peanuts\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append({\n",
    "            \"Product Title\": title,\n",
    "            \"Price\": current_price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"DE\",\n",
    "            \"Store\": \"Aldi\"\n",
    "        })\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Convert the list of products to a pandas DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Add the timestamp column to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Save to Excel (Append if file exists)\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # Load the existing Excel file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98f2de",
   "metadata": {},
   "source": [
    "### Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16487966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?query=jeden+tag+kern...\n",
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?p=1&query=jeden%20tag%20n%C3%BCsse...\n",
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?p=2&query=jeden%20tag%20n%C3%BCsse...\n",
      "Data has been successfully saved to Berrie.xlsx\n",
      "Scraping process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "url_list = [\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?query=jeden+tag+kern\",\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?p=1&query=jeden%20tag%20n%C3%BCsse\",\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?p=2&query=jeden%20tag%20n%C3%BCsse\"\n",
    "]\n",
    "\n",
    "# Get the current timestamp for CSV file\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over each URL\n",
    "for page_url in url_list:\n",
    "    print(f\"Scraping URL: {page_url}...\")\n",
    "\n",
    "    # Open the current URL\n",
    "    driver.get(page_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Loop through all product cards and extract data\n",
    "    for product_card in soup.find_all(\"div\", class_=\"product-info\"):\n",
    "        # Extract product title\n",
    "        title_tag = product_card.find(\"a\", class_=\"product-image-link product-name\")\n",
    "        title = title_tag.get(\"title\").strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract price\n",
    "        price_div = product_card.find(\"div\", class_=\"unit-price js-unit-price\")\n",
    "        price = price_div.get(\"data-value\") if price_div and price_div.has_attr(\"data-value\") else \"Price not found\"\n",
    "\n",
    "        # Extract weight\n",
    "        weight_div = product_card.find(\"div\", class_=\"price-unit-content\")\n",
    "        if weight_div:\n",
    "            # Extract only the weight part before the first parenthesis\n",
    "            weight = weight_div.text.split(\"(\")[0].strip()\n",
    "        else:\n",
    "            weight = \"Weight not found\"\n",
    "\n",
    "\n",
    "        # Extract promo price\n",
    "        promo_price = \"Promo price not found\"  # Default value in case promo price is not found\n",
    "        promo_price_div = product_card.find(\"div\", class_=\"product-price-globus-discount\")\n",
    "        if promo_price_div:\n",
    "            promo_price_element = promo_price_div.find(\"div\", class_=\"unit-price js-unit-price discount-price\")\n",
    "            if promo_price_element:\n",
    "                promo_price = promo_price_element.text.strip()\n",
    "\n",
    "        # Append the product data to the list\n",
    "        all_products.append({\n",
    "            \"Product Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"DE\",\n",
    "            \"Store\": \"Globus\",\n",
    "            \"Timestamp\": timestamp\n",
    "        })\n",
    "\n",
    "# Convert the list of products to a pandas DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Append to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # If the file exists, load the existing file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver after extracting data\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9c991-c5a8-4344-bdbe-c3ad6e0bd6ff",
   "metadata": {},
   "source": [
    "### Edeka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d8bea9-745f-48c0-9de0-d0ac6488324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://www.edeka24.de/Lebensmittel/Suess-Salzig/Chips-Knabbereien/#search:query=n%C3%BCsse+edeka&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\n",
      "Clicked 'Load More' button\n",
      "Reached end of product list.\n",
      "Scraping data from https://www.edeka24.de/Lebensmittel/Suess-Salzig/Chips-Knabbereien/#search:query=kerne+edeka&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\n",
      "Reached end of product list.\n",
      "Scraping data from https://www.edeka24.de/Lebensmittel/Suess-Salzig/Schoko-Leckereien/#search:query=alpia&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\n",
      "Reached end of product list.\n",
      "Scraping data from https://www.edeka24.de/Lebensmittel/Suess-Salzig/Nuesse-getrocknete-Fruechte/#search:query=edeka+fruchtige+mi&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\n",
      "Reached end of product list.\n",
      "Appended data to existing file: Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chrome driver with options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Disable for debugging\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.edeka24.de/Lebensmittel/Suess-Salzig/Chips-Knabbereien/#search:query=n%C3%BCsse+edeka&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\",\n",
    "    \"https://www.edeka24.de/Lebensmittel/Suess-Salzig/Chips-Knabbereien/#search:query=kerne+edeka&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\",\n",
    "    \"https://www.edeka24.de/Lebensmittel/Suess-Salzig/Schoko-Leckereien/#search:query=alpia&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\",\n",
    "    \"https://www.edeka24.de/Lebensmittel/Suess-Salzig/Nuesse-getrocknete-Fruechte/#search:query=edeka+fruchtige+mi&skipQueryLogging=true&returnResultsForLandingpages=true&first=0\"\n",
    "]\n",
    "\n",
    "# List to store all product data\n",
    "all_products = []\n",
    "\n",
    "# Function to extract weight from title\n",
    "def extract_weight(title):\n",
    "    match = re.search(r'(\\d+)\\s*(G|KG|ML|L)\\b', title, re.IGNORECASE)\n",
    "    return f\"{match.group(1)} {match.group(2).upper()}\" if match else \"Not found\"\n",
    "\n",
    "# Loop through all URLs\n",
    "for url in urls:\n",
    "    print(f\"Scraping data from {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for product elements to appear\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"product-details\")))\n",
    "    except:\n",
    "        print(f\"Products did not load properly on {url}\")\n",
    "        continue\n",
    "\n",
    "    # Keep track of previous product count to detect if more products are loading\n",
    "    previous_product_count = 0\n",
    "\n",
    "    # Click \"Load More\" button until it's no longer available\n",
    "    while True:\n",
    "        # Count the current number of products\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, \"product-details\")\n",
    "        current_product_count = len(product_elements)\n",
    "\n",
    "        # Stop clicking if no new products have loaded\n",
    "        if current_product_count == previous_product_count:\n",
    "            print(\"No more new products loaded. Stopping.\")\n",
    "            break\n",
    "\n",
    "        previous_product_count = current_product_count  # Update the count\n",
    "\n",
    "        try:\n",
    "            # Locate all buttons with ID 'loader-btn'\n",
    "            load_more_buttons = driver.find_elements(By.ID, \"loader-btn\")\n",
    "\n",
    "            button_clicked = False\n",
    "            for button in load_more_buttons:\n",
    "                button_classes = button.get_attribute(\"class\")\n",
    "\n",
    "                # Click only if it's the correct button (not 'endlist')\n",
    "                if \"button-primary\" in button_classes and \"endlist\" not in button_classes:\n",
    "                    ActionChains(driver).move_to_element(button).click().perform()\n",
    "                    print(\"Clicked 'Load More' button\")\n",
    "                    button_clicked = True\n",
    "                    break  # Exit loop after clicking\n",
    "\n",
    "            if not button_clicked:\n",
    "                print(\"Reached end of product list.\")\n",
    "                break  # Stop loop when no valid button is found\n",
    "\n",
    "            time.sleep(2)  # Wait for new products to load\n",
    "\n",
    "        except:\n",
    "            print(\"No more 'Load More' button or could not click.\")\n",
    "            break  # Exit loop if there's an exception\n",
    "\n",
    "    # Get page source after all products are loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # List to store extracted product information for this URL\n",
    "    products = []\n",
    "\n",
    "    # Loop through all product elements\n",
    "    for product_card in soup.find_all('div', class_='product-details'):\n",
    "        # Extract product title\n",
    "        title_tag = product_card.find('a', class_='title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Title not found'\n",
    "\n",
    "        # Extract weight from title\n",
    "        weight = extract_weight(title)\n",
    "\n",
    "        # Extract price and promo price\n",
    "        price_div = product_card.find('div', class_='price salesprice')  # For promo products\n",
    "        normal_price_div = product_card.find('div', class_='price')  # For normal products\n",
    "\n",
    "        promo_price = \"\"\n",
    "        price = \"0.00\"\n",
    "\n",
    "        if price_div:\n",
    "            # Extracting prices for promo products\n",
    "            price_texts = [text.strip().replace('€', '').strip() for text in price_div.stripped_strings]\n",
    "\n",
    "            if len(price_texts) == 2:\n",
    "                promo_price, price = price_texts  # First price = promo, second price = original\n",
    "            elif len(price_texts) == 1:\n",
    "                price = price_texts[0]  # Only one price found (no promo)\n",
    "        \n",
    "        elif normal_price_div:\n",
    "            # Extracting price for non-promo products\n",
    "            price = normal_price_div.get_text(strip=True).replace('€', '').strip()\n",
    "\n",
    "        # Store product details\n",
    "        products.append({\n",
    "            'Product Title': title,\n",
    "            'Price': price,\n",
    "            'Promo Price': promo_price,\n",
    "            'Weight': weight,\n",
    "            'Country': 'DE',\n",
    "            'Store': \"Edeka\"\n",
    "        })\n",
    "\n",
    "    # Add timestamp to each product\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "    for product in products:\n",
    "        product['Timestamp'] = timestamp\n",
    "\n",
    "    # Append data to global list\n",
    "    all_products.extend(products)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Save to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Append to Excel file if it exists\n",
    "if os.path.exists(excel_filename):\n",
    "    try:\n",
    "        existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Appended data to existing file: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to {excel_filename}: {e}\")\n",
    "else:\n",
    "    try:\n",
    "        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Created new file and saved data: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to {excel_filename}: {e}\")\n",
    "\n",
    "# Close the driver only once after all URLs are processed\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0f9f2",
   "metadata": {},
   "source": [
    "## Frankrijk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7e262",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a308e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi.fr/produits/epicerie-salee/biscuit-aperitif-chips.html\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=trader%20joe&searchCategory=Submitted%20Search\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=Pignons&searchCategory=Submitted%20Search\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=isaura%20choco%20peanut&searchCategory=Submitted%20Search\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        \n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        \n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        if current_price_element:\n",
    "            # Remove euro sign and convert to correct format\n",
    "            current_price = re.sub(r'[^\\d,]', '', current_price_element.get_text(strip=True))\n",
    "            current_price = current_price.replace(',', '.')  # Convert `2,99` to `2.99`\n",
    "        else:\n",
    "            current_price = 'Price not found'\n",
    "        \n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append({\n",
    "            \"Product Title\": title,\n",
    "            \"Price\": current_price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"FR\",\n",
    "            \"Store\": \"Aldi\"\n",
    "        })\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Convert price column to float if possible\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors='coerce')\n",
    "\n",
    "# Append to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # If the file exists, load the existing file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ca96a",
   "metadata": {},
   "source": [
    "### Carrefour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddda758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent handling failed\n",
      "Cookie consent handling failed\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Chrome driver with Service\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.carrefour.fr/s?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&q=melange&noRedirect=1&userIsPro=0&page=1\",\n",
    "    \"https://www.carrefour.fr/r/epicerie-sucree/sucres-farines-coulis-et-preparation-gateaux/aide-a-la-patisserie/fruits-secs-fruits-confits?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&noRedirect=0&userIsPro=0\",\n",
    "    \"https://www.carrefour.fr/r/epicerie-sucree/chocolats-et-bonbons/confiseries-chocolatees/billes-et-bonbons-au-chocolat?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&noRedirect=0&userIsPro=0\"\n",
    "]\n",
    "\n",
    "# List to store product information\n",
    "all_products = []\n",
    "\n",
    "for url in urls:\n",
    "    # Open the URL\n",
    "    time.sleep(2)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Handle cookie consent\n",
    "    try:\n",
    "        # Wait for the cookie settings button to appear\n",
    "        param_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "        param_button.click()\n",
    "\n",
    "        # Wait for and click the \"refuse all\" button\n",
    "        confirm_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "        confirm_button.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie consent handling failed\")\n",
    "\n",
    "    # Parse page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product details\n",
    "    for product_pod in soup.find_all(\"div\", class_=\"main-layout__info-zone\"):\n",
    "        # Extract title\n",
    "        title_tag = product_pod.find(\"a\", class_=\"product-card-title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract weight\n",
    "        weight_tag = product_pod.find(\"p\", class_=\"pl-text--size-m\")\n",
    "        weight = weight_tag.text.strip() if weight_tag else \"Weight not found\"\n",
    "\n",
    "        # Extract current price (main price)\n",
    "        price_main_tag = product_pod.find(\"div\", class_=\"product-price__amount--main\")\n",
    "        if price_main_tag:\n",
    "            price_main_parts = price_main_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "            if len(price_main_parts) >= 2:\n",
    "                current_price = f\"{price_main_parts[0].text.strip()}{price_main_parts[1].text.strip()} €\"\n",
    "            else:\n",
    "                current_price = \"Price not found\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "\n",
    "        # Extract promotional price\n",
    "        promo_price_tag = product_pod.find(\"div\", class_=\"product-price__amount--old\")\n",
    "        if promo_price_tag:\n",
    "            promo_price_parts = promo_price_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "            if len(promo_price_parts) >= 2:\n",
    "                promo_price = f\"{promo_price_parts[0].text.strip()},{promo_price_parts[1].text.strip()} €\"\n",
    "            else:\n",
    "                promo_price = \"Promo price not found\"\n",
    "        else:\n",
    "            promo_price = \"Promo price not found\"\n",
    "\n",
    "        # Add static values\n",
    "        Country = \"FR\"\n",
    "        Store = \"Carrefour\"\n",
    "\n",
    "        # Append extracted information to the list\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare the data for saving\n",
    "df = pd.DataFrame(all_products, columns=[\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "else:\n",
    "    # If the file doesn't exist, save the new data as a new Excel file\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7dc72",
   "metadata": {},
   "source": [
    "## Polen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2a03e",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de85769-aa11-43e7-8222-7ff42973cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi.pl/szukaj.html?query=orzechy%20trader&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=asia&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/nasze-produkty/przekaski/pestki--nasiona--ziarna.html\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=trader%20joe%27s%20&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=orzeszki%20trader&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=rodzynki&searchCategory=Suggested%20Search\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=Orzechy%20laskowe%2FMigda%C5%82y%20w%20czekoladzie%20mlecznej&searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        Country = \"PL\"\n",
    "        Store = \"Aldi\"\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare data for saving to CSV and Excel\n",
    "df = pd.DataFrame(all_products, columns=[\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Write back the combined data to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new file with the data\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1b9d5",
   "metadata": {},
   "source": [
    "### Biedronka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfaec13d-cb53-4bd8-bea2-f1d6540cb3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wweijs\\AppData\\Local\\Temp\\ipykernel_5900\\1287327326.py:57: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  integer_part = price_main_tag.find(text=True, recursive=False).strip() if price_main_tag else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/ or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Magnetic+w+czekoladzie or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Wawel+%C5%9Aliwki+w+czekoladzie+180g or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Baitz+Milk+Cookie+Balls+Koekjes+in+Melkchocolade+75+g or took too long to load\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/orzeszki/\",\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Magnetic+w+czekoladzie\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Wawel+%C5%9Aliwki+w+czekoladzie+180g\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Baitz+Milk+Cookie+Balls+Koekjes+in+Melkchocolade+75+g\"\n",
    "]\n",
    "\n",
    "# List to store all product information across multiple pages\n",
    "all_products = []\n",
    "\n",
    "# Loop over each URL\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the cookie consent button to be clickable (increased timeout)\n",
    "        param_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "        param_button.click()\n",
    "\n",
    "        # Wait for and click the button to confirm cookie consent\n",
    "        confirm_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "        confirm_button.click()\n",
    "    except TimeoutException:\n",
    "        print(f\"Cookie consent not found for URL: {url} or took too long to load\")\n",
    "\n",
    "    # Parse page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product information for the current page\n",
    "    for product_pod in soup.find_all(\"div\", class_=\"product-tile js-product-tile\"):\n",
    "        # Extract title\n",
    "        title_tag = product_pod.find(\"div\", class_=\"product-tile__name product-tile__name--overflow\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract weight (only the weight value, e.g., \"0.2kg\")\n",
    "        weight_tag = product_pod.find(\"div\", class_=\"packaging-details\")\n",
    "        if weight_tag:\n",
    "            weight = weight_tag.contents[0].strip()  # Get the first part before the <span> tag\n",
    "        else:\n",
    "            weight = \"Weight not found\"\n",
    "        \n",
    "        # Extract current price (main price)\n",
    "        price_main_tag = product_pod.find(\"div\", class_=\"price-tile__sales\")\n",
    "        if price_main_tag:\n",
    "            # Extract the integer part of the price\n",
    "            integer_part = price_main_tag.find(text=True, recursive=False).strip() if price_main_tag else None\n",
    "            decimal_part = price_main_tag.find(\"span\", class_=\"price-tile__decimal\")\n",
    "            if integer_part and decimal_part:\n",
    "                # Combine integer and decimal parts into one properly formatted price\n",
    "                raw_price = f\"{integer_part.strip()}{decimal_part.text.strip()}\"  # Combine without formatting\n",
    "                if len(raw_price) > 2:\n",
    "                    current_price = f\"{raw_price[:-2]}.{raw_price[-2:]}\"  # Insert decimal point two digits from the end\n",
    "                else:\n",
    "                    current_price = f\"0,{raw_price}\"  # Handle cases where price is less than 1 zł\n",
    "            else:\n",
    "                current_price = \"Price not found\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "\n",
    "        # Remove any extra spaces (just in case)\n",
    "        current_price = current_price.replace(\" \", \"\").strip()\n",
    "\n",
    "        # Extract promo price if available\n",
    "        promo_price_tag = product_pod.find(\"div\", class_=\"product-tile-prices__regular\")\n",
    "        if promo_price_tag:\n",
    "            promo_price = promo_price_tag.find(\"span\", class_=\"product-tile-prices__amount\")\n",
    "            if promo_price:\n",
    "                promo_price = promo_price.text.strip()\n",
    "            else:\n",
    "                promo_price = \"Promo Price not found\"\n",
    "        else:\n",
    "            promo_price = \"Promo Price not found\"\n",
    "\n",
    "        Country = \"PL\"\n",
    "        Store = \"Biedronka\"\n",
    "\n",
    "        # Append extracted information to the list\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare the data for saving\n",
    "df = pd.DataFrame(all_products, columns=[\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "else:\n",
    "    # If the file doesn't exist, save the new data as a new Excel file\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edaf73-d33f-4599-ac64-fa310266d644",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53b1ae4-4749-4fbb-9280-29a9e32c1fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://www.action.com/nl-nl/search/?q=choco+moment\n",
      "Scraping data from https://www.action.com/nl-nl/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/nl-nl/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Appended data to existing file: Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.action.com/nl-nl/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/nl-nl/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/nl-nl/search/?q=natural+happiness\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=natural+happiness\",\n",
    "    \"https://www.action.com/de-de/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/de-de/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/de-de/search/?q=natural+happiness\",    \n",
    "    \"https://www.action.com/pl-pl/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/pl-pl/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/pl-pl/search/?q=natural+happiness\"\n",
    "]\n",
    "\n",
    "# List to store all product data\n",
    "all_products = []\n",
    "\n",
    "# Loop through all URLs\n",
    "for url in urls:\n",
    "    print(f\"Scraping data from {url}\")\n",
    "    \n",
    "    # Extract the country code from the URL\n",
    "    country = url.split(\"https://www.action.com/\")[1].split(\"/\")[0][:2]\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for page load\n",
    "\n",
    "    # Accept cookies if the popup appears\n",
    "    try:\n",
    "        accept_button = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinDeclineAll\"))\n",
    "        )\n",
    "        accept_button.click()\n",
    "    except Exception:\n",
    "        print(\"No cookies popup found.\")\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Parse the page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # List to store extracted product information for this URL\n",
    "    products = []\n",
    "\n",
    "    # Loop through all product elements\n",
    "    for product_card in soup.find_all('a', {'data-testid': 'product-card-link'}):\n",
    "        # Extract product title\n",
    "        title_tag = product_card.find('span', {'data-testid': 'product-card-title'})\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Title not found'\n",
    "\n",
    "        # Extract product description\n",
    "        description_tag = product_card.find('span', {'data-testid': 'product-card-description'})\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'Description not found'\n",
    "\n",
    "        # Extract price whole part\n",
    "        price_whole_tag = product_card.find('span', {'data-testid': 'product-card-price-whole'})\n",
    "        price_whole = price_whole_tag.get_text(strip=True) if price_whole_tag else '0'\n",
    "\n",
    "        # Extract price fractional part\n",
    "        price_fractional_tag = product_card.find('span', {'data-testid': 'product-card-price-fractional'})\n",
    "        price_fractional = price_fractional_tag.get_text(strip=True) if price_fractional_tag else '00'\n",
    "\n",
    "        # Combine whole and fractional prices\n",
    "        price = f\"{price_whole}.{price_fractional}\"\n",
    "\n",
    "        # Extract price per kilo (if available)\n",
    "        priceperkilo_tag = product_card.find('span', {'data-testid': 'product-card-price-description'})\n",
    "        priceperkilo = priceperkilo_tag.get_text(strip=True) if priceperkilo_tag else 'Weight not found'\n",
    "\n",
    "        # Extract product code from image URL\n",
    "        image_tag = product_card.find('img', {'data-testid': 'product-card-image'})\n",
    "        product_code = 'Code not found'\n",
    "        if image_tag and 'src' in image_tag.attrs:\n",
    "            image_url = image_tag['src']\n",
    "            match = re.search(r\"/(\\d+)_\", image_url)\n",
    "            if match:\n",
    "                product_code = match.group(1)\n",
    "\n",
    "        # Store product details\n",
    "        products.append({\n",
    "            'Product Title': title,\n",
    "            'Price': price,\n",
    "            'Promo Price': \"\",  # Placeholder, since no promo price is extracted here\n",
    "            'Weight': description,  # Reusing description for weight\n",
    "            'Country': country,\n",
    "            'Store': \"Action\"\n",
    "        })\n",
    "\n",
    "    # Add the current URL and timestamp to each product\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "    for product in products:\n",
    "        product['Timestamp'] = timestamp\n",
    "\n",
    "    # Add the products for this URL to the overall list\n",
    "    all_products.extend(products)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Save to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Append to Excel file if it exists\n",
    "if os.path.exists(excel_filename):\n",
    "    try:\n",
    "        existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Appended data to existing file: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to {excel_filename}: {e}\")\n",
    "else:\n",
    "    try:\n",
    "        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Created new file and saved data: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to {excel_filename}: {e}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63e324-e65b-4546-ad17-58b1b3dae0bb",
   "metadata": {},
   "source": [
    "### Albert Heijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810480ec-53c0-4932-8aef-91828210288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data succesvol opgeslagen naar Berrie.xlsx in blad 'AH'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instellen van User-Agent\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "\n",
    "# Chrome opties configureren\n",
    "options = uc.options.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "\n",
    "# Start de WebDriver met undetected_chromedriver\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# URL die je wilt scrapen\n",
    "url = \"https://www.ah.nl/producten/chips-noten-toast-popcorn/noten?merk=AH&page=6\"\n",
    "\n",
    "# Ga naar de pagina\n",
    "driver.get(url)\n",
    "\n",
    "# Wacht een paar seconden zodat de pagina volledig laadt\n",
    "time.sleep(random.randint(3, 5))\n",
    "\n",
    "# Verkrijg de HTML van de pagina\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse de HTML met BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Lijst om productdata op te slaan\n",
    "products = []\n",
    "\n",
    "# Loop door alle productartikelen op de pagina\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract prijs\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    price = price_span.get('aria-label') if price_span else 'N/A'\n",
    "    \n",
    "    # Verwijder \"Prijs: €\" en extra spaties\n",
    "    if price != 'N/A':\n",
    "        price = re.sub(r'Prijs:\\s*€\\s*', '', price)  # Verwijder \"Prijs: €\"\n",
    "        price = price.strip()  # Verwijder extra spaties rondom de prijs\n",
    "\n",
    "    # Extract promo prijs\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    promo_price = \"N/A\"\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            promo_price = promo_price_span_inner.get('aria-label')\n",
    "\n",
    "    # Verwijder promo prijs \"Prijs: €\" en extra spaties indien nodig\n",
    "    if promo_price != \"N/A\":\n",
    "        promo_price = re.sub(r'Prijs:\\s*€\\s*', '', promo_price)\n",
    "        promo_price = promo_price.strip()  # Verwijder extra spaties rondom de promo prijs\n",
    "\n",
    "    # Extract titel\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'N/A'\n",
    "\n",
    "    # Extract gewicht\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'N/A'\n",
    "\n",
    "    # Voeg de verkregen data toe aan de lijst\n",
    "    products.append((title, price, promo_price, weight, \"NL\", \"AH\"))\n",
    "\n",
    "# Verkrijg de huidige timestamp voor wanneer de data werd gescrapet\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Bestandsnaam en sheetnaam\n",
    "file_name = 'Berrie.xlsx'\n",
    "sheet_name = 'AH'\n",
    "\n",
    "# Laad of maak een nieuw werkboek aan\n",
    "if os.path.exists(file_name):\n",
    "    workbook = load_workbook(file_name)\n",
    "    sheet = workbook[sheet_name] if sheet_name in workbook.sheetnames else workbook.create_sheet(sheet_name)\n",
    "else:\n",
    "    workbook = Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = sheet_name\n",
    "\n",
    "# Schrijf de header als het een nieuw blad is\n",
    "if sheet.max_row == 1:\n",
    "    sheet.append(['Product Title', 'Price', 'Promo Price', 'Weight', 'Country', 'Store', 'Timestamp'])\n",
    "\n",
    "# Voeg de productdata toe\n",
    "for product in products:\n",
    "    sheet.append([*product, timestamp])\n",
    "\n",
    "# Sla het Excel-bestand op\n",
    "workbook.save(file_name)\n",
    "print(f\"✅ Data succesvol opgeslagen naar {file_name} in blad '{sheet_name}'.\")\n",
    "\n",
    "# Sluit de browser na het scrapen\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96530dbe-da1a-484c-b6ab-1477c00e6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data succesvol opgeslagen naar Berrie.xlsx in blad 'AH'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instellen van User-Agent\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "\n",
    "# Chrome opties configureren\n",
    "options = uc.options.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "\n",
    "# Start de WebDriver met undetected_chromedriver\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# URL die je wilt scrapen\n",
    "url = \"https://www.ah.nl/producten/snoep-chocolade-koek/chocolade/chocoladesnoepjes?merk=AH&kenmerk=prijsfavoriet\"\n",
    "\n",
    "# Ga naar de pagina\n",
    "driver.get(url)\n",
    "\n",
    "# Wacht een paar seconden zodat de pagina volledig laadt\n",
    "time.sleep(random.randint(3, 5))\n",
    "\n",
    "# Verkrijg de HTML van de pagina\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse de HTML met BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Lijst om productdata op te slaan\n",
    "products = []\n",
    "\n",
    "# Loop door alle productartikelen op de pagina\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract prijs\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    price = price_span.get('aria-label') if price_span else 'N/A'\n",
    "    \n",
    "    # Verwijder \"Prijs: €\" en extra spaties\n",
    "    if price != 'N/A':\n",
    "        price = re.sub(r'Prijs:\\s*€\\s*', '', price)  # Verwijder \"Prijs: €\"\n",
    "        price = price.strip()  # Verwijder extra spaties rondom de prijs\n",
    "\n",
    "    # Extract promo prijs\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    promo_price = \"N/A\"\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            promo_price = promo_price_span_inner.get('aria-label')\n",
    "\n",
    "    # Verwijder promo prijs \"Prijs: €\" en extra spaties indien nodig\n",
    "    if promo_price != \"N/A\":\n",
    "        promo_price = re.sub(r'Prijs:\\s*€\\s*', '', promo_price)\n",
    "        promo_price = promo_price.strip()  # Verwijder extra spaties rondom de promo prijs\n",
    "\n",
    "    # Extract titel\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'N/A'\n",
    "\n",
    "    # Extract gewicht\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'N/A'\n",
    "\n",
    "    # Voeg de verkregen data toe aan de lijst\n",
    "    products.append((title, price, promo_price, weight, \"NL\", \"AH\"))\n",
    "\n",
    "# Verkrijg de huidige timestamp voor wanneer de data werd gescrapet\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Bestandsnaam en sheetnaam\n",
    "file_name = 'Berrie.xlsx'\n",
    "sheet_name = 'AH'\n",
    "\n",
    "# Laad of maak een nieuw werkboek aan\n",
    "if os.path.exists(file_name):\n",
    "    workbook = load_workbook(file_name)\n",
    "    sheet = workbook[sheet_name] if sheet_name in workbook.sheetnames else workbook.create_sheet(sheet_name)\n",
    "else:\n",
    "    workbook = Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = sheet_name\n",
    "\n",
    "# Schrijf de header als het een nieuw blad is\n",
    "if sheet.max_row == 1:\n",
    "    sheet.append(['Product Title', 'Price', 'Promo Price', 'Weight', 'Country', 'Store', 'Timestamp'])\n",
    "\n",
    "# Voeg de productdata toe\n",
    "for product in products:\n",
    "    sheet.append([*product, timestamp])\n",
    "\n",
    "# Sla het Excel-bestand op\n",
    "workbook.save(file_name)\n",
    "print(f\"✅ Data succesvol opgeslagen naar {file_name} in blad '{sheet_name}'.\")\n",
    "\n",
    "# Sluit de browser na het scrapen\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787b71e2-3216-4bdd-b950-33ec8e0ea5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data succesvol opgeslagen naar Berrie.xlsx in blad 'AH'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instellen van User-Agent\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n",
    "\n",
    "# Chrome opties configureren\n",
    "options = uc.options.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "\n",
    "# Start de WebDriver met undetected_chromedriver\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# URL die je wilt scrapen\n",
    "url = \"https://www.ah.nl/producten/chips-noten-toast-popcorn/zoutjes/rijstzoutjes\"\n",
    "\n",
    "# Ga naar de pagina\n",
    "driver.get(url)\n",
    "\n",
    "# Wacht een paar seconden zodat de pagina volledig laadt\n",
    "time.sleep(random.randint(3, 5))\n",
    "\n",
    "# Verkrijg de HTML van de pagina\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse de HTML met BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Lijst om productdata op te slaan\n",
    "products = []\n",
    "\n",
    "# Loop door alle productartikelen op de pagina\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract prijs\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    price = price_span.get('aria-label') if price_span else 'N/A'\n",
    "    \n",
    "    # Verwijder \"Prijs: €\" en extra spaties\n",
    "    if price != 'N/A':\n",
    "        price = re.sub(r'Prijs:\\s*€\\s*', '', price)  # Verwijder \"Prijs: €\"\n",
    "        price = price.strip()  # Verwijder extra spaties rondom de prijs\n",
    "\n",
    "    # Extract promo prijs\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    promo_price = \"N/A\"\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            promo_price = promo_price_span_inner.get('aria-label')\n",
    "\n",
    "    # Verwijder promo prijs \"Prijs: €\" en extra spaties indien nodig\n",
    "    if promo_price != \"N/A\":\n",
    "        promo_price = re.sub(r'Prijs:\\s*€\\s*', '', promo_price)\n",
    "        promo_price = promo_price.strip()  # Verwijder extra spaties rondom de promo prijs\n",
    "\n",
    "    # Extract titel\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'N/A'\n",
    "\n",
    "    # Extract gewicht\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'N/A'\n",
    "\n",
    "    # Voeg de verkregen data toe aan de lijst\n",
    "    products.append((title, price, promo_price, weight, \"NL\", \"AH\"))\n",
    "\n",
    "# Verkrijg de huidige timestamp voor wanneer de data werd gescrapet\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Bestandsnaam en sheetnaam\n",
    "file_name = 'Berrie.xlsx'\n",
    "sheet_name = 'AH'\n",
    "\n",
    "# Laad of maak een nieuw werkboek aan\n",
    "if os.path.exists(file_name):\n",
    "    workbook = load_workbook(file_name)\n",
    "    sheet = workbook[sheet_name] if sheet_name in workbook.sheetnames else workbook.create_sheet(sheet_name)\n",
    "else:\n",
    "    workbook = Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = sheet_name\n",
    "\n",
    "# Schrijf de header als het een nieuw blad is\n",
    "if sheet.max_row == 1:\n",
    "    sheet.append(['Product Title', 'Price', 'Promo Price', 'Weight', 'Country', 'Store', 'Timestamp'])\n",
    "\n",
    "# Voeg de productdata toe\n",
    "for product in products:\n",
    "    sheet.append([*product, timestamp])\n",
    "\n",
    "# Sla het Excel-bestand op\n",
    "workbook.save(file_name)\n",
    "print(f\"✅ Data succesvol opgeslagen naar {file_name} in blad '{sheet_name}'.\")\n",
    "\n",
    "# Sluit de browser na het scrapen\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
