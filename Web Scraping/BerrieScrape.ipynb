{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0484a7ef",
   "metadata": {},
   "source": [
    "## Nederland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b85aaa",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a45ddcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re  # Import regex library for cleaning\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://www.aldi.nl/zoeken.html?query=noten&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/producten/chips-noten/noten-zaden-en-pitten.html\",\n",
    "    \"https://www.aldi.nl/producten/chips-noten/zoutjes.html\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=pitten&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=cashew&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=trader%20joe%20amandelen%20walnoten&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=trader%20joe%20pinda&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=dry%20roasted&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=rozijnen%20&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=macadamia&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=time4choco&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=rijstzoutjes&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi.nl/zoeken.html?query=Chocoladepinda%27s&searchCategory=Submitted%20Search\"\n",
    "        \n",
    "    ]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        # Clean the weight to remove any dots\n",
    "        weight = re.sub(r'\\.', '', weight)\n",
    "\n",
    "        all_products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": current_price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"NL\",\n",
    "            \"Store\": \"Aldi\"\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Save to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d294d-15fe-4534-b44e-97622157a828",
   "metadata": {},
   "source": [
    "## Dirk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fa94ec-c165-4549-a563-5b84b9b9354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import openpyxl  # For Excel file handling\n",
    "import os  # For checking if file exists\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.dirk.nl/zoeken/producten/1%20de%20beste%20choco\",\n",
    "    \"https://www.dirk.nl/zoeken/producten/pijnboompit\",\n",
    "    \"https://www.dirk.nl/boodschappen/snacks-snoep/noten-pindas\"\n",
    "]\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"Berrie.xlsx\"\n",
    "\n",
    "# Check if the Excel file already exists\n",
    "if os.path.exists(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    ws = wb.active\n",
    "else:\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Products\"\n",
    "    ws.append([\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Retailer\", \"Timestamp\"])\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Loop over each URL\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(10)  # Wait for the page to load\n",
    "\n",
    "    # Scrape the page source\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product information\n",
    "    for article in soup.find_all('article', attrs={'data-product-id': True}):\n",
    "        title = article.find('p', class_='title').get_text(strip=True) if article.find('p', class_='title') else 'Title not found'\n",
    "        price_integer = article.find('span', class_='price-large')\n",
    "        price_decimals = article.find('span', class_='price-small')\n",
    "        price = f\"{price_integer.get_text(strip=True)},{price_decimals.get_text(strip=True)}\" if price_integer and price_decimals else 'Price not found'\n",
    "        promo_price_span = article.find('div', class_='label price-label')\n",
    "        promo_price = promo_price_span.find('span', class_='regular-price').find('span').get_text(strip=True) if promo_price_span else 'Promo price not found'\n",
    "        weight_span = article.find('span', class_='subtitle')\n",
    "        weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "        \n",
    "        # Write product data to Excel\n",
    "        ws.append([title, price, promo_price, weight, \"NL\", \"Dirk\", timestamp])\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_name)\n",
    "print(f\"Data has been successfully saved to {file_name}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e50c5-ad6c-4e15-8735-9f041cd257cf",
   "metadata": {},
   "source": [
    "## Vomar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ee3a6f-42d0-4009-9261-2b11951ad672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accept cookies button found.\n",
      "No accept cookies button found.\n",
      "No accept cookies button found.\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import openpyxl  # For Excel file handling\n",
    "import os  # For checking if file exists\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.vomar.nl/zoeken?search=g%27woon%20choco\",\n",
    "    \"https://www.vomar.nl/zoeken?search=noten\",\n",
    "    \"https://www.vomar.nl/zoeken?search=pitten\",\n",
    "    \"https://www.vomar.nl/zoeken?search=rijstzoutjes\"\n",
    "]\n",
    "\n",
    "products = []\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Allow time for page to load\n",
    "    \n",
    "    # Click the \"Weigeren\" button to reject cookies on the Vomar site\n",
    "    try:\n",
    "        deny_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"CybotCookiebotDialogBodyButtonDecline\")))\n",
    "        deny_button.click()\n",
    "    except:\n",
    "        print(\"No accept cookies button found.\")\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Loop through all product articles\n",
    "    for article in soup.find_all('div', class_='col-xs-12 col-md-3 product'):\n",
    "        title = article.find('p', class_='description').get_text(strip=True) if article.find('p', class_='description') else 'Title not found'\n",
    "\n",
    "        price_integer = article.find('span', class_='large')\n",
    "        price_decimals = article.find('span', class_='small')\n",
    "        \n",
    "        if price_integer and price_decimals:\n",
    "            price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "        else:\n",
    "            price = 'Price not found'\n",
    "\n",
    "        promo_price = 'Promo price not found'\n",
    "        weight = 'Weight not found'\n",
    "        \n",
    "        products.append((title, price, promo_price, weight, \"NL\", \"Vomar\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"Berrie.xlsx\"\n",
    "\n",
    "# Check if the Excel file already exists\n",
    "if os.path.exists(file_name):\n",
    "    wb = openpyxl.load_workbook(file_name)\n",
    "    ws = wb.active\n",
    "else:\n",
    "    wb = openpyxl.Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Products\"\n",
    "    ws.append([\"Product Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Retailer\", \"Timestamp\"])\n",
    "\n",
    "# Write product data to Excel\n",
    "for product in products:\n",
    "    ws.append((*product, timestamp))\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "wb.save(file_name)\n",
    "\n",
    "print(f\"Data has been successfully saved to {file_name}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683471e",
   "metadata": {},
   "source": [
    "## Duitsland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25a7d",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2405a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=asiatisce%20snack&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=kerne&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/sortiment/snacks-suessigkeiten/nuesse-trockenfruechte.html\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=trader%20joe%20n%C3%BCsse&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=trader%20joe%20mix&searchCategory=Submitted%20Search\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?query=schoko%20rosinen&searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi-nord.de/suchergebnisse.html?searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&query=choceur%20peanuts\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": current_price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"DE\",\n",
    "        \"Store\": \"Aldi\"\n",
    "        })\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Convert the list of products to a pandas DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Add the timestamp column to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Save to Excel (Append if file exists)\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # Load the existing Excel file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98f2de",
   "metadata": {},
   "source": [
    "### Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16487966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?query=jeden+tag+kern...\n",
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?p=1&query=jeden%20tag%20n%C3%BCsse...\n",
      "Scraping URL: https://produkte.globus.de/bobenheim-roxheim/search?p=2&query=jeden%20tag%20n%C3%BCsse...\n",
      "Data has been successfully saved to Berrie.xlsx\n",
      "Scraping process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime  # Importing datetime module\n",
    "import pandas as pd  # Importing pandas for Excel file saving\n",
    "import os  # Importing os to check if file exists\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "url_list = [\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?query=jeden+tag+kern\",\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?p=1&query=jeden%20tag%20n%C3%BCsse\",\n",
    "    \"https://produkte.globus.de/bobenheim-roxheim/search?p=2&query=jeden%20tag%20n%C3%BCsse\"\n",
    "]\n",
    "\n",
    "# Get the current timestamp for CSV file\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over each URL\n",
    "for page_url in url_list:\n",
    "    print(f\"Scraping URL: {page_url}...\")\n",
    "\n",
    "    # Open the current URL\n",
    "    driver.get(page_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Loop through all product cards and extract data\n",
    "    for product_card in soup.find_all(\"div\", class_=\"product-info\"):\n",
    "        # Extract product title\n",
    "        title_tag = product_card.find(\"a\", class_=\"product-image-link product-name\")\n",
    "        title = title_tag.get(\"title\").strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract price\n",
    "        price_div = product_card.find(\"div\", class_=\"unit-price js-unit-price\")\n",
    "        price = price_div.get(\"data-value\") if price_div and price_div.has_attr(\"data-value\") else \"Price not found\"\n",
    "\n",
    "        # Extract weight\n",
    "        weight_div = product_card.find(\"div\", class_=\"price-unit-content\")\n",
    "        if weight_div:\n",
    "            # Extract only the weight part before the first parenthesis\n",
    "            weight = weight_div.text.split(\"(\")[0].strip()\n",
    "        else:\n",
    "            weight = \"Weight not found\"\n",
    "\n",
    "\n",
    "        # Extract promo price\n",
    "        promo_price = \"Promo price not found\"  # Default value in case promo price is not found\n",
    "        promo_price_div = product_card.find(\"div\", class_=\"product-price-globus-discount\")\n",
    "        if promo_price_div:\n",
    "            promo_price_element = promo_price_div.find(\"div\", class_=\"unit-price js-unit-price discount-price\")\n",
    "            if promo_price_element:\n",
    "                promo_price = promo_price_element.text.strip()\n",
    "\n",
    "        # Append the product data to the list\n",
    "        all_products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"DE\",\n",
    "            \"Store\": \"Globus\",\n",
    "            \"Timestamp\": timestamp\n",
    "        })\n",
    "\n",
    "# Convert the list of products to a pandas DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Append to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # If the file exists, load the existing file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver after extracting data\n",
    "driver.quit()\n",
    "\n",
    "print(\"Scraping process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0f9f2",
   "metadata": {},
   "source": [
    "## Frankrijk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7e262",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a308e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import os\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi.fr/produits/epicerie-salee/biscuit-aperitif-chips.html\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=trader%20joe&searchCategory=Submitted%20Search\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=Pignons&searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_fr_fr_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_fr_fr_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_fr_fr_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_fr_fr_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_fr_fr_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_fr_fr_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_fr_fr_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_fr_fr_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.fr/recherche.html?query=isaura%20choco%20peanut&searchCategory=Submitted%20Search\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append({\n",
    "            \"Title\": title,\n",
    "            \"Price\": current_price,\n",
    "            \"Promo Price\": promo_price,\n",
    "            \"Weight\": weight,\n",
    "            \"Country\": \"FR\",\n",
    "        \"Store\": \"Aldi\"\n",
    "        })\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "\n",
    "# Append to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "if os.path.exists(excel_filename):\n",
    "    # If the file exists, load the existing file and append the new data\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new one\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ca96a",
   "metadata": {},
   "source": [
    "### Carrefour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ddda758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent handling failed\n",
      "Cookie consent handling failed\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.carrefour.fr/s?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&q=melange&noRedirect=1&userIsPro=0&page=1\",\n",
    "    \"https://www.carrefour.fr/r/epicerie-sucree/sucres-farines-coulis-et-preparation-gateaux/aide-a-la-patisserie/fruits-secs-fruits-confits?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&noRedirect=0&userIsPro=0\",\n",
    "    \"https://www.carrefour.fr/r/epicerie-sucree/chocolats-et-bonbons/confiseries-chocolatees/billes-et-bonbons-au-chocolat?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&noRedirect=0&userIsPro=0\"\n",
    "]\n",
    "\n",
    "# List to store product information\n",
    "all_products = []\n",
    "\n",
    "for url in urls:\n",
    "    # Open the URL\n",
    "    time.sleep(2)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Handle cookie consent\n",
    "    try:\n",
    "        # Wait for the cookie settings button to appear\n",
    "        param_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "        param_button.click()\n",
    "\n",
    "        # Wait for and click the \"refuse all\" button\n",
    "        confirm_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "        confirm_button.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Cookie consent handling failed\")\n",
    "\n",
    "    # Parse page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product details\n",
    "    for product_pod in soup.find_all(\"div\", class_=\"main-layout__info-zone\"):\n",
    "        # Extract title\n",
    "        title_tag = product_pod.find(\"a\", class_=\"product-card-title\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract weight\n",
    "        weight_tag = product_pod.find(\"p\", class_=\"pl-text--size-m\")\n",
    "        weight = weight_tag.text.strip() if weight_tag else \"Weight not found\"\n",
    "\n",
    "        # Extract current price (main price)\n",
    "        price_main_tag = product_pod.find(\"div\", class_=\"product-price__amount--main\")\n",
    "        if price_main_tag:\n",
    "            price_main_parts = price_main_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "            if len(price_main_parts) >= 2:\n",
    "                current_price = f\"{price_main_parts[0].text.strip()}{price_main_parts[1].text.strip()} €\"\n",
    "            else:\n",
    "                current_price = \"Price not found\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "\n",
    "        # Extract promotional price\n",
    "        promo_price_tag = product_pod.find(\"div\", class_=\"product-price__amount--old\")\n",
    "        if promo_price_tag:\n",
    "            promo_price_parts = promo_price_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "            if len(promo_price_parts) >= 2:\n",
    "                promo_price = f\"{promo_price_parts[0].text.strip()},{promo_price_parts[1].text.strip()} €\"\n",
    "            else:\n",
    "                promo_price = \"Promo price not found\"\n",
    "        else:\n",
    "            promo_price = \"Promo price not found\"\n",
    "\n",
    "        # Add static values\n",
    "        Country = \"FR\"\n",
    "        Store = \"Carrefour\"\n",
    "\n",
    "        # Append extracted information to the list\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare the data for saving\n",
    "df = pd.DataFrame(all_products, columns=[\"Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "else:\n",
    "    # If the file doesn't exist, save the new data as a new Excel file\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7dc72",
   "metadata": {},
   "source": [
    "## Polen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2a03e",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3de85769-aa11-43e7-8222-7ff42973cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.aldi.pl/szukaj.html?query=orzechy%20trader&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=asia&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/nasze-produkty/przekaski/pestki--nasiona--ziarna.html\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=trader%20joe%27s%20&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=orzeszki%20trader&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=rodzynki&searchCategory=Suggested%20Search\",\n",
    "    \"https://www.aldi.pl/szukaj.html?query=Orzechy%20laskowe%2FMigda%C5%82y%20w%20czekoladzie%20mlecznej&searchCategory=Submitted%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\"\n",
    "]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        Country = \"PL\"\n",
    "        Store = \"Aldi\"\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare data for saving to CSV and Excel\n",
    "df = pd.DataFrame(all_products, columns=[\"Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Write back the combined data to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "else:\n",
    "    # If the file doesn't exist, create a new file with the data\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1b9d5",
   "metadata": {},
   "source": [
    "### Biedronka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfaec13d-cb53-4bd8-bea2-f1d6540cb3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wweijs\\AppData\\Local\\Temp\\ipykernel_1364\\1598713035.py:71: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  integer_part = price_main_tag.find(text=True, recursive=False).strip() if price_main_tag else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/ or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Magnetic+w+czekoladzie or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Wawel+%C5%9Aliwki+w+czekoladzie+180g or took too long to load\n",
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/search?q=Baitz+Milk+Cookie+Balls+Koekjes+in+Melkchocolade+75+g or took too long to load\n",
      "Data has been successfully saved to Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/orzeszki/\",\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Magnetic+w+czekoladzie\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Wawel+%C5%9Aliwki+w+czekoladzie+180g\",\n",
    "    \"https://zakupy.biedronka.pl/search?q=Baitz+Milk+Cookie+Balls+Koekjes+in+Melkchocolade+75+g\"\n",
    "]\n",
    "\n",
    "# List to store all product information across multiple pages\n",
    "all_products = []\n",
    "\n",
    "# Loop over each URL\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the cookie consent button to be clickable (increased timeout)\n",
    "        param_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "        param_button.click()\n",
    "\n",
    "        # Wait for and click the button to confirm cookie consent\n",
    "        confirm_button = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "        confirm_button.click()\n",
    "    except TimeoutException:\n",
    "        print(f\"Cookie consent not found for URL: {url} or took too long to load\")\n",
    "\n",
    "    # Parse page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product information for the current page\n",
    "    for product_pod in soup.find_all(\"div\", class_=\"product-tile js-product-tile\"):\n",
    "        # Extract title\n",
    "        title_tag = product_pod.find(\"div\", class_=\"product-tile__name product-tile__name--overflow\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract weight (only the weight value, e.g., \"0.2kg\")\n",
    "        weight_tag = product_pod.find(\"div\", class_=\"packaging-details\")\n",
    "        if weight_tag:\n",
    "            weight = weight_tag.contents[0].strip()  # Get the first part before the <span> tag\n",
    "        else:\n",
    "            weight = \"Weight not found\"\n",
    "        \n",
    "        # Extract current price (main price)\n",
    "        price_main_tag = product_pod.find(\"div\", class_=\"price-tile__sales\")\n",
    "        if price_main_tag:\n",
    "            # Extract the integer part of the price\n",
    "            integer_part = price_main_tag.find(text=True, recursive=False).strip() if price_main_tag else None\n",
    "            decimal_part = price_main_tag.find(\"span\", class_=\"price-tile__decimal\")\n",
    "            if integer_part and decimal_part:\n",
    "                # Combine integer and decimal parts into one properly formatted price\n",
    "                raw_price = f\"{integer_part.strip()}{decimal_part.text.strip()}\"  # Combine without formatting\n",
    "                if len(raw_price) > 2:\n",
    "                    current_price = f\"{raw_price[:-2]}.{raw_price[-2:]}\"  # Insert decimal point two digits from the end\n",
    "                else:\n",
    "                    current_price = f\"0,{raw_price}\"  # Handle cases where price is less than 1 zł\n",
    "            else:\n",
    "                current_price = \"Price not found\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "\n",
    "        # Remove any extra spaces (just in case)\n",
    "        current_price = current_price.replace(\" \", \"\").strip()\n",
    "\n",
    "        # Extract promo price if available\n",
    "        promo_price_tag = product_pod.find(\"div\", class_=\"product-tile-prices__regular\")\n",
    "        if promo_price_tag:\n",
    "            promo_price = promo_price_tag.find(\"span\", class_=\"product-tile-prices__amount\")\n",
    "            if promo_price:\n",
    "                promo_price = promo_price.text.strip()\n",
    "            else:\n",
    "                promo_price = \"Promo Price not found\"\n",
    "        else:\n",
    "            promo_price = \"Promo Price not found\"\n",
    "\n",
    "        Country = \"PL\"\n",
    "        Store = \"Biedronka\"\n",
    "\n",
    "        # Append extracted information to the list\n",
    "        all_products.append((title, current_price, promo_price, weight, Country, Store))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Prepare the data for saving\n",
    "df = pd.DataFrame(all_products, columns=[\"Title\", \"Price\", \"Promo Price\", \"Weight\", \"Country\", \"Store\"])\n",
    "\n",
    "# Add timestamp to the DataFrame\n",
    "df[\"Timestamp\"] = timestamp\n",
    "\n",
    "# Excel file name\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the Excel file exists\n",
    "if os.path.exists(excel_filename):\n",
    "    # Read the existing data from the Excel file\n",
    "    existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "\n",
    "    # Append the new data to the existing data\n",
    "    combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the combined data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "else:\n",
    "    # If the file doesn't exist, save the new data as a new Excel file\n",
    "    df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been successfully saved to {excel_filename}\")\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edaf73-d33f-4599-ac64-fa310266d644",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c53b1ae4-4749-4fbb-9280-29a9e32c1fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from https://www.action.com/nl-nl/search/?q=choco+moment\n",
      "Scraping data from https://www.action.com/nl-nl/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/nl-nl/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/fr-fr/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/de-de/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=choco+moment\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=snacks+of+the+world\n",
      "No cookies popup found.\n",
      "Scraping data from https://www.action.com/pl-pl/search/?q=natural+happiness\n",
      "No cookies popup found.\n",
      "Appended data to existing file: Berrie.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import os  # For checking if the file exists\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.action.com/nl-nl/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/nl-nl/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/nl-nl/search/?q=natural+happiness\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/fr-fr/search/?q=natural+happiness\",\n",
    "    \"https://www.action.com/de-de/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/de-de/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/de-de/search/?q=natural+happiness\",    \n",
    "    \"https://www.action.com/pl-pl/search/?q=choco+moment\",\n",
    "    \"https://www.action.com/pl-pl/search/?q=snacks+of+the+world\",\n",
    "    \"https://www.action.com/pl-pl/search/?q=natural+happiness\"\n",
    "]\n",
    "\n",
    "# List to store all product data\n",
    "all_products = []\n",
    "\n",
    "# Loop through all URLs\n",
    "for url in urls:\n",
    "    print(f\"Scraping data from {url}\")\n",
    "    \n",
    "    # Extract the country code from the URL\n",
    "    country = url.split(\"https://www.action.com/\")[1].split(\"/\")[0][:2]\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Accept cookies if the popup appears\n",
    "    try:\n",
    "        accept_button = WebDriverWait(driver, 3).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinDeclineAll\"))\n",
    "        )\n",
    "        accept_button.click()\n",
    "    except Exception as e:\n",
    "        print(\"No cookies popup found.\")\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Parse the page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # List to store extracted product information for this URL\n",
    "    products = []\n",
    "\n",
    "    # Loop through all product articles\n",
    "    for article in soup.find_all('article', {'data-testid': 'product-card'}):\n",
    "        # Extract product title\n",
    "        title_tag = article.find('p', {'data-testid': 'product-card-title'})\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'Title not found'\n",
    "\n",
    "        # Extract product description (optional)\n",
    "        description_tag = article.find('p', {'data-testid': 'product-card-description'})\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'Description not found'\n",
    "\n",
    "        # Extract price whole part\n",
    "        price_whole_tag = article.find('span', {'data-testid': 'product-card-price-whole'})\n",
    "        price_whole = price_whole_tag.get_text(strip=True) if price_whole_tag else '0'\n",
    "\n",
    "        # Extract price fractional part\n",
    "        price_fractional_tag = article.find('span', {'data-testid': 'product-card-price-fractional'})\n",
    "        price_fractional = price_fractional_tag.get_text(strip=True) if price_fractional_tag else '00'\n",
    "\n",
    "        # Combine whole and fractional prices\n",
    "        price = f\"{price_whole}.{price_fractional}\"\n",
    "\n",
    "        # Extract priceperkilo (if available)\n",
    "        priceperkilo_tag = article.find('span', {'data-testid': 'product-card-price-description'})\n",
    "        priceperkilo = priceperkilo_tag.get_text(strip=True) if priceperkilo_tag else 'Weight not found'\n",
    "\n",
    "        # Store product details for this URL\n",
    "        products.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Promo Price': \"\",  # Placeholder, since no promo price is extracted here\n",
    "            'Weight': description,  # Reusing description for weight\n",
    "            'Country': country,\n",
    "            'Store': \"Action\"\n",
    "        })\n",
    "\n",
    "    # Add the current URL and timestamp to each product\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "    for product in products:\n",
    "        product['Timestamp'] = timestamp\n",
    "\n",
    "    # Add the products for this URL to the overall list\n",
    "    all_products.extend(products)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_products)\n",
    "\n",
    "# Save to Excel file\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Append to Excel file if it exists\n",
    "if os.path.exists(excel_filename):\n",
    "    try:\n",
    "        existing_df = pd.read_excel(excel_filename, engine='openpyxl')\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        updated_df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Appended data to existing file: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending to {excel_filename}: {e}\")\n",
    "else:\n",
    "    try:\n",
    "        df.to_excel(excel_filename, index=False, engine='openpyxl')\n",
    "        print(f\"Created new file and saved data: {excel_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to {excel_filename}: {e}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63e324-e65b-4546-ad17-58b1b3dae0bb",
   "metadata": {},
   "source": [
    "### Albert Heijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "810480ec-53c0-4932-8aef-91828210288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to a new sheet in the existing workbook.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re  # For regular expressions\n",
    "from datetime import datetime  # For timestamps\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from openpyxl import load_workbook  # For appending to existing Excel file\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ah.nl/producten/chips-noten-toast-popcorn/noten?merk=AH&page=6\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Accept cookies (specific to the website)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "# Get page source and parse with BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available)\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Append data to products list\n",
    "    products.append({\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Promo Price\": promo_price,\n",
    "        \"Weight\": weight,\n",
    "        \"Country\": \"NL\",\n",
    "        \"Store\": \"AH\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the products list\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Define Excel filename\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "# Check if the file already exists\n",
    "try:\n",
    "    # Try to open the existing workbook and append the new data to a new sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='a') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=f\"AH\")\n",
    "    print(\"Data has been successfully saved to a new sheet in the existing workbook.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create a new workbook and save the data\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False)\n",
    "    print(\"New Excel file created and data saved.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96530dbe-da1a-484c-b6ab-1477c00e6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully appended to the existing sheet.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re  # For regular expressions\n",
    "from datetime import datetime  # For timestamps\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from openpyxl import load_workbook  # For handling existing Excel files\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ah.nl/producten/snoep-chocolade-koek/chocolade/chocoladesnoepjes?merk=AH&kenmerk=prijsfavoriet\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Accept cookies (specific to the website)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "# Get page source and parse with BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available)\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Append data to products list\n",
    "    products.append({\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Promo Price\": promo_price,\n",
    "        \"Weight\": weight,\n",
    "        \"Country\": \"NL\",\n",
    "        \"Store\": \"AH\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the products list\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Define Excel filename\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load existing workbook\n",
    "    book = load_workbook(excel_filename)\n",
    "    \n",
    "    # Check if sheet exists\n",
    "    if \"AH\" in book.sheetnames:\n",
    "        # Load existing sheet into a DataFrame\n",
    "        existing_data = pd.read_excel(excel_filename, sheet_name=\"AH\")\n",
    "        \n",
    "        # Concatenate existing and new data\n",
    "        combined_data = pd.concat([existing_data, df], ignore_index=True)\n",
    "    else:\n",
    "        # If sheet doesn't exist, just use new data\n",
    "        combined_data = df\n",
    "    \n",
    "    # Write the updated data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        combined_data.to_excel(writer, index=False, sheet_name=\"AH\")\n",
    "    \n",
    "    print(\"Data successfully appended to the existing sheet.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create a new workbook and save the data\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=\"AH\")\n",
    "    print(\"New Excel file created and data saved.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "787b71e2-3216-4bdd-b950-33ec8e0ea5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully appended to the existing sheet.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re  # For regular expressions\n",
    "from datetime import datetime  # For timestamps\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from openpyxl import load_workbook  # For handling existing Excel files\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.ah.nl/producten/chips-noten-toast-popcorn/zoutjes/rijstzoutjes\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Accept cookies (specific to the website)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "# Get page source and parse with BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available)\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Append data to products list\n",
    "    products.append({\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Promo Price\": promo_price,\n",
    "        \"Weight\": weight,\n",
    "        \"Country\": \"NL\",\n",
    "        \"Store\": \"AH\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the products list\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Add a timestamp column\n",
    "df[\"Timestamp\"] = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Define Excel filename\n",
    "excel_filename = 'Berrie.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load existing workbook\n",
    "    book = load_workbook(excel_filename)\n",
    "    \n",
    "    # Check if sheet exists\n",
    "    if \"AH\" in book.sheetnames:\n",
    "        # Load existing sheet into a DataFrame\n",
    "        existing_data = pd.read_excel(excel_filename, sheet_name=\"AH\")\n",
    "        \n",
    "        # Concatenate existing and new data\n",
    "        combined_data = pd.concat([existing_data, df], ignore_index=True)\n",
    "    else:\n",
    "        # If sheet doesn't exist, just use new data\n",
    "        combined_data = df\n",
    "    \n",
    "    # Write the updated data back to the same sheet\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        combined_data.to_excel(writer, index=False, sheet_name=\"AH\")\n",
    "    \n",
    "    print(\"Data successfully appended to the existing sheet.\")\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create a new workbook and save the data\n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=\"AH\")\n",
    "    print(\"New Excel file created and data saved.\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
